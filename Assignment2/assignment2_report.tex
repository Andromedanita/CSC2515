\documentclass[10pt]{article} 

\usepackage{rotating}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{graphicx}
\usepackage[numbers,square,sort&compress]{natbib}
\usepackage{setspace}
\usepackage[cdot,mediumqspace,]{SIunits}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{url}
\usepackage{authblk}
\usepackage{placeins}
\usepackage{float}



\onehalfspacing
\title{Assignment 2}
\author{Anita Bahmanyar}
\date{November 14, 2016}


\begin{document}

\maketitle


\section*{Question 3}

% Q3.1
\subsection*{3.1}
We want to show the result of the runs for both NN and CNN methods with the default parameter values. The figures are shown below:
% nn
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{Q31_nn_report.png}
	\caption{NN method used, eps=0.01, momentum=0.0, batch size=100}
	\label{fig:}
\end{figure}

% cnn
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{cnn_eps01.png}
	\caption{CNN method used, eps=0.1, momentum=0.0, batch size=100}
	\label{fig:}
\end{figure}

As it is shown in the figures, validation set accuracy is always smaller than the training set which makes sense since we are training the model based on the training set and not on the validation set. As a result, the cross-entropy of validation set is also always larger than that of the training set meaning the model works better for the training set than the validation (since smaller cross-entropy means better results).


% Q3.2
\subsection*{3.2}
\subsubsection*{NN}
This part of the assignment is asking for running the neural network code with different values of hyper parameters. The first part is to fix all the hyper parameters and change epsilon values from 0.001 to 1.0 for 5 different epsilon values.
Figure \ref{fig:Q3_valid_combined} shows the cross entropy and accuracy of validation set for all 5 different epsilon values which I chose to be 0.001, 0.01, 0.1, 0.5 and 1.0.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{Q3_report_valid.png}
	\caption{}
	\label{fig:Q3_valid_combined}
\end{figure}
What we conclude is that if epsilon is very small do instance in the case of 0.001, it takes very long for the validation to converge which means longer computation time. Also, if epsilon value is very large, for instance 0.5 and 1.0, then the accuracy curve would not be improving and it stays around 0.3 since we take large steps and we miss the global optimum value. I found that epsilon=0.01 and epsilon=0.1 have similar values and since the initialization is random, it is hard to say which is better since the accuracy of using these values are pretty similar and for each run they differ due to the randomness. The value of epsilon I choose is \textbf{epsilon=0.01} to do the other runs and I keep it 0.01 which changing other parameters.

In Figures \ref{fig:refer1} and \ref{fig:refer2} I show few different plots that are for the same epsilon values but they are separate plots for each epsilon so that we can see the training curve as well.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{nn_CE_eps01.png}
	\caption{\textbf{epsilon=0.01}, CE on the left and accuracy on the right for both validation and training sets.}
	\label{fig:refer1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{nn_CE_eps10.png}
	\caption{\textbf{epsilon=1.0}, CE on the left and accuracy on the right for both validation and training sets.}
	\label{fig:refer2}
\end{figure}

We also see that the accuracy of training set is always better than validation set which makes sense since we are training the model based on the training set.
This is also true for cross-entropy where the training set always has smaller CE compared to the validation set.


In this part, I keep epsilon the same (0.01) and only change momentum value to be 0.0, 0.5 and 0.9. As it is shown in Figures \ref{fig} below, we see that momentum=0.9 does not return good results and we conclude it is too large. The results of momentum=0.0 and momentum=0.5 are similar except for the cross-entropy where for the case of momentum=0.5, the validation cross-entropy stars to go up at a much earlier time compared to momentum=0.0. When the validation CE starts increasing instead of decreasing that is the point where training further does not help which means it does not matter if we train more except that we spend more computational time, so the momentum value that I think is better in this case is \textbf{momentum=0.0} and I will use this value for the next part of the question.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{nn_acc_momentum_all.png}
	\caption{\textbf{epsilon=1.0}, CE on the left and accuracy on the right for both validation and training sets for three different momentum values while keeping \textbf{epsilon=0.01}. The momentum values 	are 0.0, 0.5 and 0.9 from top to the bottom.}
	\label{fig:refer2}
\end{figure}

Smaller batch sizes take longer to run due to having more steps to run since it is defined in the code that \begin{verbatim} num_steps = int(np.ceil(num_train_cases / batch_size)) \end{verbatim} so smaller batch size means more steps.

% batch = 1
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch1.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=1}}
	\label{fig:Q32_batch10}
\end{figure}

% batch =10
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch10.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=10}}
	\label{fig:Q32_batch10}
\end{figure}

% batch = 100
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch100.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=100}}
	\label{fig:Q32_batch10}
\end{figure}

% batch = 500
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch500.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=500}}
	\label{fig:Q32_batch10}
\end{figure}

% batch = 1000
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch1000.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=1000}}
	\label{fig:Q32_batch10}
\end{figure}

What I conclude is that the smaller batch size results in worse accuracy and bad cross-entropies. The higher batch size values result in higher accuracies and lower cross-entropies up to batch size=100. Also, for the case of batch size=1000, accuracy of validation set is very close to that of training set as well their cross entropies. This is because the training accuracies become lower. Also, for batch size values higher than 100, the accuracies converge slower. So the best value for \textbf{batch size=100}.

%%%% CNN %%%%%
\subsubsection*{CNN}
I ran CNN code for five different epsilon values and then compared their accuracies and their cross-entropies. Below are the results for each run.
% eps = 0.001
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_eps0001.png}
	\caption{\textbf{epsilon=0.001, momentum=0.0, batch size=100}}
	\label{fig:Q32_batch10}
\end{figure}


% eps = 0.01
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_eps001.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}


% eps = 0.1
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_eps01.png}
	\caption{\textbf{epsilon=0.1, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}


% eps = 0.5
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_eps05.png}
	\caption{\textbf{epsilon=0.5, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}


% eps = 1.0
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_eps1.png}
	\caption{\textbf{epsilon=1.0, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}

Based on the figures above, \textbf{eps = 0.1} returns the best results using CNN method.

% changing momentum for CNN
Fixing \textbf{eps = 0.1}, I ran CNN code using three different momentum values and the results are shown in figures below:

% momentum = 0.0
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_eps01.png}
	\caption{\textbf{epsilon=0.1, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}


% momentum = 0.5
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_mom05.png}
	\caption{\textbf{epsilon=0.1, momentum=0.5, batch size=100}}
	\label{fig:}
\end{figure}


% momentum = 0.9
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_mom09.png}
	\caption{\textbf{epsilon=0.1, momentum=0.9, batch size=100}}
	\label{fig:}
\end{figure}

As we see from the figures above, the best model is a result of using \textbf{momentum=0.0}. Using momentum=0.9 results in very bad accuracy and cross-entropy and using momentum=0.5 has lower accuracy and higher CE than when the momentum is 0.

% batch = 1
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_batch1.png}
	\caption{\textbf{epsilon=0.1, momentum=0.0, batch size=1}}
	\label{fig:}
\end{figure}


% batch = 10
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_batch10.png}
	\caption{\textbf{epsilon=0.1, momentum=0.0, batch size=10}}
	\label{fig:}
\end{figure}

% batch = 100
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_batch100.png}
	\caption{\textbf{epsilon=0.1, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}


% batch = 500
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_batch500.png}
	\caption{\textbf{epsilon=0.1, momentum=0.0, batch size=500}}
	\label{fig:}
\end{figure}


% batch = 1000
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_acc_batch1000.png}
	\caption{\textbf{epsilon=0.1, momentum=0.0, batch size=1000}}
	\label{fig:}
\end{figure}

Looking at the figures above, we see that batch size=100 has the best results since the accuracy of the validation set is higher and it has lower cross entropy. Batch size=500 and batch size=1000 have lower accuracies around 0.45 -0.6 which is lower than 0.8 for the 100 batch size. Accuracy for batch size = 10 is about 0.6 and stays the same the whole time with no improvement and batch size=1 generates accuracy of as most 0.3 and does not improve either and has high cross-entropy above 1.5.




%%%%  Q3.3 %%%%
\subsection*{3.3}
\subsubsection*{NN}
Keeping the momentum fixed and equal to 0.9, and having eps = 0.01 for NN, below are the results I got when changing the number of hidden units for each layer. The three combinations I tried are [16,32], [32,32] and [32,16] to see if it matters if the first layer has more units, or if it matters if they are the same or the second layer has more units. Results are shown below:

% [16,32]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q33_hidden_16_32.png}
	\caption{\textbf{[16,32],epsilon=0.01, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}


% [32,32]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q33_hidden_32_32.png}
	\caption{\textbf{[32,32],epsilon=0.01, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}


% [32,16]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q33_hidden_32_16.png}
	\caption{\textbf{[32,16],epsilon=0.01, momentum=0.0, batch size=100}}
	\label{fig:}
\end{figure}

As we see above, the accuracy of the validation set is more or less the same when using different number of units in the hidden layers and the small variation could be due to the randomness of the weight initialization. Cross-entropy of validation set starts increasing when there is the same number of units in both 


\subsubsection*{CNN}


% [8,16]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_8_16_bad.png}
	\caption{\textbf{[8,16],epsilon=0.1, momentum=0.9, batch size=100}}
	\label{fig:}
\end{figure}


% [16,16]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_16_16_bad.png}
	\caption{\textbf{[16,16],epsilon=0.1, momentum=0.9, batch size=100}}
	\label{fig:}
\end{figure}


% [16,8]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_16_8_bad.png}
	\caption{\textbf{[16,8],epsilon=0.1, momentum=0.9, batch size=100}}
	\label{fig:}
\end{figure}


We see that learning rate of eps = 0.1 is too high when using momentum=0.9 in this case which caused the model to miss the optimum value and not learn and not improve.This means that we need to change eps to be smaller so I chose \textbf{eps=0.1, momentum=0.9} and then tried these filters again. The new results are shown below:

% new can filter results

% [8,16]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_8_16.png}
	\caption{\textbf{[8,16],epsilon=0.01, momentum=0.9, batch size=100}}
	\label{fig:}
\end{figure}


% [16,16]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_16_16.png}
	\caption{\textbf{[16,16],epsilon=0.01, momentum=0.9, batch size=100}}
	\label{fig:}
\end{figure}


% [16,8]
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{cnn_16_8.png}
	\caption{\textbf{[16,8],epsilon=0.01, momentum=0.9, batch size=100}}
	\label{fig:}
\end{figure}

We see that we get much better results when I adjusted the learning rate. Also, as we increase the number of hidden units, the code runs slower because it has to do more calculations. Larger number of hidden units result in better values but too large of value results in over-fitting. Too small of number of hidden units results in not capturing all of the features as well. So we need to decide on the number of hidden units such that it does not over-fit or does not miss capturing features.




% Q3.4
\subsection*{3.4}
The number of parameters we need to calculate is the obtained in the following way: \\
size(W1) + size(W2) + size(W3) + size(b1) + size(b2) + size(b3). So I printed the size of these arrays in the initialization function in both NN and CNN. \\ 
For NN, the size of each weight and bias is as follows: \\ 
size(W1) = $2304 \times N_1$ \\
size(W2) = $N_1  \times N_2$ \\
size(W3) = $N_2  \times 7$ \\
size(b1)  = $N_1$ \\ 
size(b2)  = $N_2$ \\ 
size(b3)  = 7 \\

For CNN case, the weights and bias sizes are given as follows: \\
size(W1) = $5 \times 5 \times 1 \times N_1$\\
size(W2) = $5 \times 5 \times N_1 \times N_2$ \\
size(W3) = $64 \times N_2$ \\
size(b1)  = $N_1$ \\ 
size(b2)  = $N_2$ \\ 
size(b3)  = 7 \\

So I wrote a function to sum these up and then tried a few different values of $N_1$ and $N_2$ for can and then tried to get a close value for nn with trial and error. The code is shown below:

\begin{verbatim}
def func_cnn(N1, N2):
	return 26*N1 + 449*N2 + 25*N1*N2 + 7
	
def func_nn(N1, N2):	
	return 2305*N1 + 8*N2 + 7 +N1*N2
\end{verbatim}

The values I got for nn code are [$N_1,N_2$] = [7,13] which results in 16337 parameters (including bias) and for can I choose [$N_1, N2_$] = [10,23] which results in 16344 parameters (including bias). The number of parameters are very similar.

The sum of all of these would be the number of parameters in each method, so we need to choose $N_1$ and $N_2$ values for each method such that both methods have similar number f parameters so that we can compare the results better.

Using these values for the number of hidden values and filters, I got the results shown below:

% nn, CE, acc
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{Q34_nn_7_13.png}
	\caption{Cross-entropy and accuracy using nn method using hidden layers = [7,13].}
	\label{fig:}
\end{figure}


% nn, imshow
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{Q34_nn_imshow_7_13.png}
	\caption{Weights of the first layer using nn method using hidden layers = [7,13].}
	\label{fig:}
\end{figure}


% cnn, CE, acc
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{Q34_cnn_10_23.png}
	\caption{Cross-entropy and accuracy using nn method using hidden layers = [10,23].}
	\label{fig:}
\end{figure}


% cnn, imshow
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{Q34_cnn_imshow_10_23.png}
	\caption{Weights of the first layer using cnn method using hidden layers = [10,23].}
	\label{fig:}
\end{figure}

The first layer we are looking at for the nn method, we can see human face fewtures in the images. We can see nose and eyes in some of them which means that in nn method it tries to find parts of the whole image in this case looks for eyes or nose. However, in cnn method,  we can see that we cannot distinguish any face features and we can only see vertical and horizontal lines which means that this layer is looking for vertical and horizontal lines only. This is the difference between the two methods.


% Q3.5
\subsection*{3.5}

% Q4
\section*{Question 4}
\subsection*{4.2}
In the code provided (mogEM.py), inside function mogEM there are a few lines that include "randConst" value.
\begin{verbatim}
p  = randConst + np.random.rand(K, 1)
p  = p / np.sum(p)   # mixing coefficients
mu = mn + np.random.randn(N, K) * (np.sqrt(vr) / randConst)
\end{verbatim}
The first line will shows $\pi_k$ values which are the mixing coefficients in the Gaussian mixture method. The mixing coefficients will be dominated by random values if randConst is small and it will be dominated by the value of randConst if it is large.
In the third line, as we increase randConst, "(np.sqrt(vr) / randConst)" decreases and as we decrease the randConst value, this part of "mu" expression increases. This means that as we increase the randConst value, the "mu" values will be dominated by the mean values since the second part of "mu" expression will be very small.


% Q4.2 log-likelihood for all randConst values
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/Q2_logL.png}
	\caption{Log-likelihood as a function of number of iterations for different values of randConst parameter.}
	\label{fig:q4_log_iter}
\end{figure}
As we see in Figure \ref{fig:q4_log_iter}, randConst=1.0 converges faster to the similar log-likelihood values compared to the other randConst values. So the model I choose has randConst=1.0.
Below are variance and mean of the images shown for randConst = 1.0:
% Q4.2 mu
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q2_mu_1.png}
	\caption{Mean of the images- Is is blurry because it is average.}
	\label{fig:q4.2_mu}
\end{figure}

% Q4.2 variance
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q2_vary_1.png}
	\caption{Variance of the images-black means lower variance and white means higher variance.}
	\label{fig:q4.2_vary}
\end{figure}

We see that most of these images look the same to us even though they are not and this is because there are small variations in the images. So variances are the most helpful ones in this case to see the variation between images.
Also, black in the variance images mean lower variance and whiter means higher variance. Areas with lower variance are better so it is better if we see a lot of black areas in the variance images. Lower variance means better classifications so the variance images with more black area are better classifiers.

Below in Figure \ref{fig:q4.2_pik}, the values of Gaussian mixture coefficients are given vs. cluster number.
% Q4.2 mixture coefficients
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.45]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q2_Mixcoeffbar_1.png}
	\caption{Value of mixture of coefficients for each cluster.}
	\label{fig:q4.2_pik}
\end{figure}


\subsection*{4.3}
Initializing the means using K-means makes the code converge much faster than just using random values for the means.
This can be seen in Figure \ref{fig:q4.3_mu} below.

% Q4.3 log-likelihood for randConst=1.0
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/kmeans_random_compare}
	\caption{Comparison between log-likelihood using two different methods: Kmeans shown in magenta and randomized method (using randConst) shown in blue.
	We can see that using means method the log-likelihood converges earlier to a similar value compared to the randomized method.}
	\label{fig:q4.3_logL}
\end{figure}


% Q4.3 mu
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q3_mu_1.png}
	\caption{}
	\label{fig:q4.3_mu}
\end{figure}


% Q4.3 variance
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q3_vary_1.png}
	\caption{}
	\label{fig:q4.3_mu}
\end{figure}

% Q4.4
\subsection*{4.4}
In this question, we are trying to compute $p(d|x)$ value using Baye's rule. using Baye's rule we know that:
\begin{equation}\label{eq:bayes}
p(d|x) = \frac{p(x|d) p(d)}{p(x)},
\end{equation}
where $p(x|d)$, $p(d)$ is the prior and $p(x)$ is the evidence. Since $p(x)$ is constant for all the values we can ignore it here. Then, we can take log of both sides of Equation (\ref{eq:bayes}) and write it as:
\begin{equation}
\mathrm{log} (p(d|x)) = \mathrm{log}(p(x|d) p(d)) = \mathrm{log}(p(x|d)) + \mathrm{log}(p(d)).
\end{equation} 
We do have value of $p(d)$ from "log\_likelihood\_class" function in the code provided and value of $p(x|d)$ is given in the function called "mogLogLikelihood". So we can use these functions to compute \mathrm{log}$p(d|x)$.

Figure \ref{fig:err_rate} shows the for all three cases of training set, validation set and test set.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/Q4.4/q4_randConst_1.png}
	\caption{}
	\label{fig:4.4}
\end{figure}



Answers to the questions: \\

(b) We find that the error rates on the training set generally decreases as the number of clusters increases. This is because it is easier to fit to larger number of Gaussians. It obviously over-fits the training data if the number of cluster is very large but it still fits the training set well so the error rate decreases. \\

(c) The error rate for test set decreases as we increase the number of clusters. This is not the case for validation set as we see (the error rate increases for higher cluster numbers). This is because we are over-fitting the training set by having larger number of clusters and therefore the model does not work well on the validation set for larger number of clusters and the reason it works for the test set is because of the input we have for test set. I think we are just lucky that it works well on the test set and usually this is not the case.


\end{document}

