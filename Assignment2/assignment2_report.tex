\documentclass[10pt]{article} 

\usepackage{rotating}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{graphicx}
\usepackage[numbers,square,sort&compress]{natbib}
\usepackage{setspace}
\usepackage[cdot,mediumqspace,]{SIunits}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{url}
\usepackage{authblk}
\usepackage{placeins}
\usepackage{float}



\onehalfspacing
\title{Assignment 2}
\author{Anita Bahmanyar}
\date{November 14, 2016}


\begin{document}

\maketitle


\section*{Question 3}
\subsection*{3.1}

% Q3.2
\subsection*{3.2}
This part of the assignment is asking for running the neural network code with different values of hyper parameters. The first part is to fix all the hyper parameters and change epsilon values from 0.001 to 1.0 for 5 different epsilon values.
Figure \ref{fig:Q3_valid_combined} shows the cross entropy and accuracy of validation set for all 5 different epsilon values which I chose to be 0.001, 0.01, 0.1, 0.5 and 1.0.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{Q3_report_valid.png}
	\caption{}
	\label{fig:Q3_valid_combined}
\end{figure}
What we conclude is that if epsilon is very small do instance in the case of 0.001, it takes very long for the validation to converge which means longer computation time. Also, if epsilon value is very large, for instance 0.5 and 1.0, then the accuracy curve would not be improving and it stays around 0.3 since we take large steps and we miss the global optimum value. I found that epsilon=0.01 and epsilon=0.1 have similar values and since the initialization is random, it is hard to say which is better since the accuracy of using these values are pretty similar and for each run they differ due to the randomness. The value of epsilon I choose is \textbf{epsilon=0.01} to do the other runs and I keep it 0.01 which changing other parameters.

In Figures \ref{fig:refer1} and \ref{fig:refer2} I show few different plots that are for the same epsilon values but they are separate plots for each epsilon so that we can see the training curve as well.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{nn_CE_eps01.png}
	\caption{\textbf{epsilon=0.01}, CE on the left and accuracy on the right for both validation and training sets.}
	\label{fig:refer1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{nn_CE_eps10.png}
	\caption{\textbf{epsilon=1.0}, CE on the left and accuracy on the right for both validation and training sets.}
	\label{fig:refer2}
\end{figure}

We also see that the accuracy of training set is always better than validation set which makes sense since we are training the model based on the training set.
This is also true for cross-entropy where the training set always has smaller CE compared to the validation set.


In this part, I keep epsilon the same (0.01) and only change momentum value to be 0.0, 0.5 and 0.9. As it is shown in Figures \ref{fig} below, we see that momentum=0.9 does not return good results and we conclude it is too large. The results of momentum=0.0 and momentum=0.5 are similar except for the cross-entropy where for the case of momentum=0.5, the validation cross-entropy stars to go up at a much earlier time compared to momentum=0.0. When the validation CE starts increasing instead of decreasing that is the point where training further does not help which means it does not matter if we train more except that we spend more computational time, so the momentum value that I think is better in this case is \textbf{momentum=0.0} and I will use this value for the next part of the question.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{nn_acc_momentum_all.png}
	\caption{\textbf{epsilon=1.0}, CE on the left and accuracy on the right for both validation and training sets for three different momentum values while keeping \textbf{epsilon=0.01}. The momentum values 	are 0.0, 0.5 and 0.9 from top to the bottom.}
	\label{fig:refer2}
\end{figure}

Smaller batch sizes take longer to run due to having more steps to run since it is defined in the code that \begin{verbatim} num_steps = int(np.ceil(num_train_cases / batch_size)) \end{verbatim} so smaller batch size means more steps.

% batch = 1
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch11.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=1}}
	\label{fig:Q32_batch10}
\end{figure}

% batch =10
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch10.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=10}}
	\label{fig:Q32_batch10}
\end{figure}

% batch = 100
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch100.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=100}}
	\label{fig:Q32_batch10}
\end{figure}

% batch = 500
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch500.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=500}}
	\label{fig:Q32_batch10}
\end{figure}

% batch = 1000
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Q32_batch1000.png}
	\caption{\textbf{epsilon=0.01, momentum=0.0, batch size=1000}}
	\label{fig:Q32_batch10}
\end{figure}



% Q3.3
\subsection*{3.3}

% Q3.4
\subsection*{3.4}

% Q3.5
\subsection*{3.5}

% Q4
\section*{Question 4}
\subsection*{4.2}
In the code provided (mogEM.py), inside function mogEM there are a few lines that include "randConst" value.
\begin{verbatim}
p  = randConst + np.random.rand(K, 1)
p  = p / np.sum(p)   # mixing coefficients
mu = mn + np.random.randn(N, K) * (np.sqrt(vr) / randConst)
\end{verbatim}
The first line will shows $\pi_k$ values which are the mixing coefficients in the Gaussian mixture method. The mixing coefficients will be dominated by random values if randConst is small and it will be dominated by the value of randConst if it is large.
In the third line, as we increase randConst, "(np.sqrt(vr) / randConst)" decreases and as we decrease the randConst value, this part of "mu" expression increases. This means that as we increase the randConst value, the "mu" values will be dominated by the mean values since the second part of "mu" expression will be very small.


% Q4.2 log-likelihood for all randConst values
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/Q2_logL.png}
	\caption{Log-likelihood as a function of number of iterations for different values of randConst parameter.}
	\label{fig:q4_log_iter}
\end{figure}
As we see in Figure \ref{fig:q4_log_iter}, randConst=1.0 converges faster to the similar log-likelihood values compared to the other randConst values. So the model I choose has randConst=1.0.
Below are variance and mean of the images shown for randConst = 1.0:
% Q4.2 mu
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q2_mu_1.png}
	\caption{Mean of the images- Is is blurry because it is average.}
	\label{fig:q4.2_mu}
\end{figure}

% Q4.2 variance
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q2_vary_1.png}
	\caption{Variance of the images-black means lower variance and white means higher variance.}
	\label{fig:q4.2_vary}
\end{figure}

We see that most of these images look the same to us even though they are not and this is because there are small variations in the images. So variances are the most helpful ones in this case to see the variation between images.
Also, black in the variance images mean lower variance and whiter means higher variance. Areas with lower variance are better so it is better if we see a lot of black areas in the variance images. Lower variance means better classifications so the variance images with more black area are better classifiers.

Below in Figure \ref{fig:q4.2_pik}, the values of Gaussian mixture coefficients are given vs. cluster number.
% Q4.2 mixture coefficients
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.45]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q2_Mixcoeffbar_1.png}
	\caption{Value of mixture of coefficients for each cluster.}
	\label{fig:q4.2_pik}
\end{figure}


\subsection*{4.3}
Initializing the means using K-means makes the code converge much faster than just using random values for the means.
This can be seen in Figure \ref{fig:q4.3_mu} below.

% Q4.3 log-likelihood for randConst=1.0
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/kmeans_random_compare}
	\caption{Comparison between log-likelihood using two different methods: Kmeans shown in magenta and randomized method (using randConst) shown in blue.
	We can see that using means method the log-likelihood converges earlier to a similar value compared to the randomized method.}
	\label{fig:q4.3_logL}
\end{figure}


% Q4.3 mu
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q3_mu_1.png}
	\caption{}
	\label{fig:q4.3_mu}
\end{figure}


% Q4.3 variance
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/q3_vary_1.png}
	\caption{}
	\label{fig:q4.3_mu}
\end{figure}

% Q4.4
\subsection*{4.4}
In this question, we are trying to compute $p(d|x)$ value using Baye's rule. using Baye's rule we know that:
\begin{equation}\label{eq:bayes}
p(d|x) = \frac{p(x|d) p(d)}{p(x)},
\end{equation}
where $p(x|d)$, $p(d)$ is the prior and $p(x)$ is the evidence. Since $p(x)$ is constant for all the values we can ignore it here. Then, we can take log of both sides of Equation (\ref{eq:bayes}) and write it as:
\begin{equation}
\mathrm{log} (p(d|x)) = \mathrm{log}(p(x|d) p(d)) = \mathrm{log}(p(x|d)) + \mathrm{log}(p(d)).
\end{equation} 
We do have value of $p(d)$ from "log\_likelihood\_class" function in the code provided and value of $p(x|d)$ is given in the function called "mogLogLikelihood". So we can use these functions to compute \mathrm{log}$p(d|x)$.

Figure \ref{fig:err_rate} shows the for all three cases of training set, validation set and test set.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/Q4.4/q4_randConst_1.png}
	\caption{}
	\label{fig:4.4}
\end{figure}

Just to see the results because we have some randomness in the values we start with, I ran this part of the code again and the result is shown below for the same values as Figure \ref{fig:4.4}.
Figure \ref{fig:err_rate} shows for all three cases of training set, validation set and test set.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{/Users/anita/Documents/Grad_Second_Year/CSC2515/assignment2/Results/Q4/Q4.4/q4_randConst_1_fig1.png}
	\caption{}
	\label{fig:err_rate}
\end{figure}

\end{document}

